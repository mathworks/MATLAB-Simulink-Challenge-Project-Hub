**Project 213:** Fill out this <strong>[form](https://forms.office.com/Pages/ResponsePage.aspx?id=ETrdmUhDaESb3eUHKx3B5lOTzSa_A6lPqq2LJKzvpM5UMTBZRkc4UTRETjFERVRDWllQRE40OUFSQS4u)</strong> to  register your intent to complete this project and learn about the reward

<table>
<td><img src="https://gist.githubusercontent.com/robertogl/e0115dc303472a9cfd52bbbc8edb7665/raw/visualslamPicture4.png"  width=500 /></td>
<td><p><h1>Robust Visual SLAM Using Mobile Sensor Streaming</h1></p>
<p>Perform robust visual SLAM using MATLAB Mobile sensor streaming.</p>
</table>

## Motivation

Mobiles phones are within everyone’s reach and have sensors such as IMU, magnetic compass, GPS, camera, which can be used for visual SLAM. The challenge has been in streaming multi-sensor data such as GPS and camera data in sync to enable robust visual SLAM. This could be potentially used for developing and testing SLAM and navigation algorithms (map building and path planning) by many researchers without the need for additional complex hardware and setup. Furthermore, the resulting point cloud of an object or environment could be leveraged for augmented reality applications.

## Project Description

Work with [Computer Vision Toolbox™](https://www.mathworks.com/products/communications.html) (CVT), [Navigation Toolbox™](https://www.mathworks.com/products/navigation.html) (NAV), [Sensor Fusion and Tracking Toolbox™](https://www.mathworks.com/products/sensor-fusion-and-tracking.html) (SFTT), [webcam hardware support]( https://www.mathworks.com/videos/webcam-support-89504.html )), and [MATLAB® Mobile™](https://www.mathworks.com/products/matlab-mobile.html) to enable robust visual SLAM from mobile sensors. Using the examples provided in Steps 1 and 2 below,  acquire streaming sensor data and calibrate the data. Other examples in steps 3 and 4 help build a map and export to a point cloud. This project will extend the suggested basic example and focus on integrating all of these into a complete workflow that is general enough for robust visual SLAM and works independent of the source of the streaming data.

Suggested steps:
1.	Obtain streaming sensor data from [MATLAB® Mobile™](https://www.mathworks.com/products/matlab-mobile.html) and [webcam]( https://www.mathworks.com/videos/webcam-support-89504.html )
2.	Calibrate the sensors [example](https://www.mathworks.com/help/vision/ug/camera-calibration.html).
3.	Use CVT, NAV, and SFTT to build robust visual SLAM
4.	Export resulting map to a point cloud
5.	Finally, a workflow that demonstrates robust visual SLAM can be developed.

Project variations:
-	Use MATLAB Mobile and webcam/Mobile together to obtain the sensor data
-	Upgrade MATLAB Mobile to stream camera data with integrated IMU and GPS readings
-	Crowdsourcing raw sensor data (images, GPS, IMU readings) to build large scale maps along with dynamic updates.


## Background Material

Examples:
-	[Monocular Visual SLAM](https://www.mathworks.com/help/vision/ug/monocular-visual-simultaneous-localization-and-mapping.html)
-	[Structure from Motion](https://www.mathworks.com/help/vision/ug/structure-from-motion-from-multiple-views.html)
-	[Optimize Pose Graph](https://www.mathworks.com/help/nav/ref/optimizeposegraph.html)

Suggested readings:

[1] P. Tanskanen, K. Kolev, L. Meier, F. Camposeco, O. Saurer and M. Pollefeys, "Live Metric 3D Reconstruction on Mobile Phones," 2013 IEEE International Conference on Computer Vision, 
2013, pp. 65-72, doi: 10.1109/ICCV.2013.15.


## Impact

Enable visual SLAM from streaming sensors and extend the state-of-art in real-time visual SLAM algorithms.

## Expertise Gained 

Autonomous Vehicles, Computer Vision, Drones, Robotics, Automotive, AUV, Mobile Robots, Manipulators, Humanoid, UAV, UGV


## Project Difficulty

Bachelor, Master's, Doctoral

## Project Discussion

[Dedicated discussion forum](https://github.com/mathworks/MathWorks-Excellence-in-Innovation/discussions/44) to ask/answer questions, comment, or share your ideas for solutions for this project.


## Project Number

213
